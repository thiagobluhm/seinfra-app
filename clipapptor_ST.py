import streamlit as st
import os
import fitz  # PyMuPDF
from openai import OpenAI
import re
from io import BytesIO
from dotenv import load_dotenv
from streamlit import session_state as ss
from streamlit_pdf_viewer import pdf_viewer
from tqdm import tqdm

# üîπ Carregar vari√°veis de ambiente
load_dotenv()
secretk = os.environ.get("OPENAI_API_KEY")
cliente_openai = OpenAI(api_key=secretk)

# üîπ Fun√ß√£o para extrair texto do PDF
def extract_content(pdf_bytes):
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    extracted_data = []
    for page_num, page in enumerate(doc, start=1):
        extracted_data.append({
            "page_number": page_num,
            "text": page.get_text(),
            "link": f"#page={page_num}"
        })
    return extracted_data

# üîπ Fun√ß√£o para identificar palavras-chave no texto
def find_keywords(text, keywords):
    return [kw for kw in keywords if kw.lower() in text.lower()]


def sanitize_text(text):
    """
    Remove caracteres n√£o suportados pelo formato XML usado no Word.
    """
    # Remove caracteres de controle e bytes nulos
    sanitized_text = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', text)
    
    # Garante que o texto seja Unicode
    return sanitized_text.encode('utf-8', 'ignore').decode('utf-8')

# üîπ Verifica se um texto cont√©m palavras-chave relevantes
def is_relevant(text, keywords):
    return any(keyword.lower() in text.lower() for keyword in keywords)

# üîπ Filtrar conte√∫do com base no prompt e nas palavras-chave
def filter_content(prompt, extracted_data, keywords):
    filtered_data = []

    for page in tqdm(extracted_data, desc="Classificando p√°ginas com OpenAI", unit="p√°gina"):
        if page["text"].strip():
            try:
                response = cliente_openai.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "Voc√™ √© um assistente que classifica textos extra√≠dos de PDFs."},
                        {"role": "user", "content": f"Prompt: {prompt}\nTexto: {page['text']}"}
                    ], 
                    temperature=0.3, 
                    max_tokens=4096
                )
                relevance = response.choices[0].message.content
                if "relevante" in relevance.lower() or is_relevant(page["text"], keywords):
                    filtered_data.append(page)

            except Exception as e:
                print(f"Erro ao classificar conte√∫do na p√°gina {page['page_number']}: {e}")
        elif page["images"]:  # Se n√£o h√° texto, mas h√° imagens, ainda inclu√≠mos
            filtered_data.append(page)

    return filtered_data


# üîπ Fun√ß√£o para gerar resumos via OpenAI
def generate_summary(text):
    prompt = f"Resuma o seguinte texto em poucas frases mantendo os pontos principais:\n\n{text}"
    try:
        response = cliente_openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um assistente que resume textos de forma objetiva."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=150
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Erro ao gerar resumo: {e}"

# üîπ Processamento do PDF
def process_pdf(pdf_bytes):

    # Prompt de exemplo
    prompt = """
                Voc√™ √© o analista respons√°vel pelo Clipping do √ìrg√£o SEINFRA-CE. 
                Sua tarefa √© Extrair todas as not√≠cias sobre:

                1. Governo do Estado do Cear√°: qualquer assunto, incluindo sa√∫de, infraestrutura, educa√ß√£o, seguran√ßa p√∫blica e pol√≠tica.
                2. Governo Federal: not√≠cias de infraestrutura, sa√∫de, pol√≠tica nacional e temas relevantes para √≥rg√£os p√∫blicos.
                3. Pol√≠tica Nacional: temas relacionados a governadores, prefeitos, ministros, deputados e senadores.
                4. Infraestrutura e Sa√∫de: qualquer projeto, evento ou decis√£o relacionada a esses temas.

                N√£o deixe de pegar nenhum texto importante. Priorize textos que mencionem 'Governo Federal', 'Governo do Estado do Cear√°' ou seus representantes.
            """
    keywords = [
        "Governo Federal", "Governo do Estado do Cear√°", "Cear√°", "infraestrutura",
        "sa√∫de", "educa√ß√£o", "seguran√ßa p√∫blica", "investimento", "ministro", 
        "governador", "prefeito", "pol√≠tica", "infraestrutura urbana", "mobilidade urbana",
        "transporte p√∫blico", "energia renov√°vel", "obras p√∫blicas", "hospital", 
        "escolas p√∫blicas", "recursos federais", "recursos estaduais", "moradia", 
        "habita√ß√£o", "economia", "desenvolvimento regional", "assist√™ncia social", 
        "meio ambiente", "√°gua pot√°vel", "saneamento b√°sico", "pontes", "rodovias", 
        "ferrovias", "aeroportos", "portos", "log√≠stica", "seguran√ßa alimentar", 
        "or√ßamento p√∫blico", "pol√≠ticas p√∫blicas", "emendas parlamentares", 
        "licita√ß√£o", "contratos p√∫blicos", "parcerias p√∫blico-privadas", 
        "inova√ß√£o tecnol√≥gica", "tecnologia", "sa√∫de p√∫blica", "sistema √∫nico de sa√∫de",
        "vacina√ß√£o", "doen√ßas infecciosas", "crise h√≠drica", "minist√©rio", 
        "agricultura", "turismo", "ind√∫stria", "com√©rcio exterior", "universidades"
    ]
    extracted_data = extract_content(pdf_bytes)
    print("üìë Classificando textos extra√≠dos...")
    filtered_data = filter_content(prompt, extracted_data, keywords)

    return filtered_data

# üîπ Streamlit Interface
st.set_page_config(page_title="Clipapptor - IA para Resumo de PDFs", layout="wide")
st.title("üìë Clipapptor - IA para Resumo de PDFs")

# Sidebar com Upload e Bot√£o
with st.sidebar:
    uploaded_file = st.file_uploader("üì§ Fa√ßa upload do PDF", type=["pdf"], key='pdf')
    if st.button("üìÑ Gerar Clippings do Jornal") and uploaded_file:
        with st.spinner("Processando PDF..."):
            resumos = process_pdf(uploaded_file.getvalue())
        ss.resumos = resumos  # Armazena resumos na sess√£o

# Exibir PDF ao lado
col1, col2 = st.columns([2, 3])

with col1:
    if ss.get('pdf'):
        pdf_viewer(input=ss.pdf.getvalue(), width=700)

with col2:
    if ss.get('resumos'):
        for resumo in ss.resumos:
            st.markdown(f"### P√°gina {resumo['page_number']}")
            st.write(f"**Palavras-chave:** {resumo['keywords']}")
            st.write(f"**Resumo:** {resumo['summary']}")
            st.markdown("---")
    else:
        st.warning("Aguardando gera√ß√£o de clipping...")
